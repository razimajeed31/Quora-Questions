{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load in \n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"../input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "print(os.listdir(\"../input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "45bcf9edc667abd4797d0475ca45d854e8ed26b6"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, CuDNNGRU, Conv1D, CuDNNLSTM\n",
    "from keras.layers import Bidirectional, GlobalMaxPool1D\n",
    "from keras.models import Model\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b1696d674b6b077e1ac80493d4e5fac779e77753"
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../input/train.csv')\n",
    "test_df = pd.read_csv('../input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e78dca0b94a0750d0534c3fe6a82f1ea1f714857"
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "137b67f0aa2a387ee3057df6a031d7a58f3217bb"
   },
   "outputs": [],
   "source": [
    "train_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "b0ce7b6ab36c1873d565de63ab1f302e23f693b2"
   },
   "outputs": [],
   "source": [
    "#embed_size = 300 # how big is each word vector\n",
    "max_features = 20000 # how many unique words to use (i.e num rows in embedding vector)\n",
    "maxlen = 200 # max number of words in a question to use\n",
    "train_X = train_df[\"question_text\"].fillna(\"_na_\").values\n",
    "val_X = val_df[\"question_text\"].fillna(\"_na_\").values\n",
    "test_X = test_df[\"question_text\"].fillna(\"_na_\").values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "31309996adff263cf2918e9629a70514bc7ae020"
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=max_features)\n",
    "tokenizer.fit_on_texts(list(train_X))\n",
    "train_X = tokenizer.texts_to_sequences(train_X)\n",
    "val_X = tokenizer.texts_to_sequences(val_X)\n",
    "test_X = tokenizer.texts_to_sequences(test_X)\n",
    "\n",
    "## Pad the sentences \n",
    "train_X = pad_sequences(train_X, maxlen=maxlen)\n",
    "val_X = pad_sequences(val_X, maxlen=maxlen)\n",
    "test_X = pad_sequences(test_X, maxlen=maxlen)\n",
    "\n",
    "## Get the target values\n",
    "train_y = train_df['target'].values\n",
    "val_y = val_df['target'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2c801859596c3822fbe5f6fa1e5717540ecbbdd2"
   },
   "outputs": [],
   "source": [
    "#np.random.seed(2018)\n",
    "#trn_idx = np.random.permutation(len(train_X))\n",
    "#val_idx = np.random.permutation(len(val_X))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "dacb86f33fe448f0e4c14f958e0a29be0d0bbd46"
   },
   "outputs": [],
   "source": [
    "#train_X = train_X[trn_idx]\n",
    "#val_X = val_X[val_idx]\n",
    "#train_y = train_y[trn_idx]\n",
    "#val_y = val_y[val_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "3475695932a0526d66f373651eee3a0ac62bef11"
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../input/embeddings/glove.840B.300d/glove.840B.300d.txt'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "c119586d3b6041d47ad11ed1666f40d2b3e5f46f"
   },
   "outputs": [],
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "d2a320b4e9347b626cd605928b547699f1a1e350"
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "e4d6dc59d50c1c81d9f69937ac6c75bc0e700626"
   },
   "outputs": [],
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "#x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aadce9cf9a283ee9464d04c45d289a37a1b0b26d"
   },
   "outputs": [],
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "2b532e55559d0203750beaa8f66e51458ef9bba3"
   },
   "outputs": [],
   "source": [
    "#pred_noemb_val_y = model.predict([val_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding the apporximate threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "835f816f77e1b97fa0f5cdf1f16188b65272e4c6"
   },
   "outputs": [],
   "source": [
    "#for thresh in np.arange(0.1, 0.501, 0.01):\n",
    " #   thresh = np.round(thresh, 2)\n",
    "  #  print(\"F1 score at threshold {0} is {1}\".format(thresh, metrics.f1_score(val_y, (pred_noemb_val_y>thresh).astype(int))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "f5e7f712e241d5013394a50b3671da3b838f8a68"
   },
   "outputs": [],
   "source": [
    "pred_GRU = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "ce4406a9c28afa065a5d85e1f8b727a1195d2131"
   },
   "source": [
    "del model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "48a3477da2851ccd6124e466373ba04808eb49af"
   },
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "#x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "3452cf5b22ed6c484797632fab5d752eeac57051"
   },
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "634e432836e1ff83b680669d7c82012741631540"
   },
   "source": [
    "pred_LSTM = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "6ef24a1cf82cc3ee622568b12eaedc4fd9d527a3"
   },
   "source": [
    "pred_LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "40ab843c2c827b4e93949fd6d43721f5061e64df"
   },
   "source": [
    "del word_index, embeddings_index, all_embs, embedding_matrix, model, inp, x\n",
    "import gc; gc.collect()\n",
    "time.sleep(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f51f96a338dc3225cc9ed9e287af71f2a1126200"
   },
   "source": [
    "EMBEDDING_FILE = '../input/embeddings/wiki-news-300d-1M/wiki-news-300d-1M.vec'\n",
    "def get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\n",
    "embeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "730c913fa21b0c789e57b527fd70db573081b304"
   },
   "source": [
    "all_embs = np.stack(embeddings_index.values())\n",
    "emb_mean,emb_std = all_embs.mean(), all_embs.std()\n",
    "embed_size = all_embs.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "b89cc818fc5de0405a086deeb682c0133a55e82f"
   },
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "14f22acf9c1ca5a0b2637f0d205c82a4382a4b72"
   },
   "source": [
    "inp = Input(shape=(maxlen,))\n",
    "x = Embedding(max_features, embed_size, weights=[embedding_matrix])(inp)\n",
    "#x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)\n",
    "x = Bidirectional(CuDNNLSTM(64, return_sequences=True))(x)\n",
    "x = GlobalMaxPool1D()(x)\n",
    "x = Dense(16, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(1, activation=\"sigmoid\")(x)\n",
    "model = Model(inputs=inp, outputs=x)\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "75be23cb7326bd71bf4b40b5d37c6888cb03c9f6"
   },
   "source": [
    "model.fit(train_X, train_y, batch_size=512, epochs=2, validation_data=(val_X, val_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1b435d681c4a0ab7dd6f9c5eff0446d2e1aa4d19"
   },
   "source": [
    "pred_LSTMtwo = model.predict([test_X], batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4f595859c450bd8af8543618ce4f01479a1dad09"
   },
   "source": [
    "pred_GRU = (pred_GRU>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1e62820031a7e4e68fdeb88d5dc5d23772c8622d"
   },
   "source": [
    "pred_LSTM = (pred_LSTM>0.5).astype(int)\n",
    "pred_LSTMtwo = (pred_LSTMtwo>0.5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f7809ea731a49d7ee78a81ec633049040a3dae4a"
   },
   "source": [
    "type(pred_GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_uuid": "aefc676adbb02bfb19d5fee915b78e4688d94cf8"
   },
   "outputs": [],
   "source": [
    "#pred = 0.33*pred_glove_test_y + 0.33*pred_fasttext_test_y + 0.34*pred_paragram_test_y\n",
    "#pred = 0.34*pred_LSTM + 0.33*pred_GRU+0.33*pred_LSTMtwo\n",
    "pred_test_y = (pred_GRU>0.29).astype(int)\n",
    "out_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\n",
    "out_df['prediction'] = pred_test_y\n",
    "out_df.to_csv(\"submission.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
